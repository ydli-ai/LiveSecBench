#!/usr/bin/env python3
"""
ç«¯åˆ°ç«¯æ¼”ç¤ºè„šæœ¬
é€šè¿‡ mock patchè¿”å›å›ºå®šå†…å®¹ï¼Œå®ç°ç«¯åˆ°ç«¯æ¼”ç¤º
"""

import asyncio
import os
import sys
from pathlib import Path
from unittest.mock import patch

ROOT_DIR = Path(__file__).resolve().parent.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

try:
    from livesecbench.infra.config import ConfigManager
    from livesecbench.run_livesecbench import load_questions, load_models_from_config_manager
    from livesecbench.core.run_model_answer import batch_gen_llm_answer
    from livesecbench.core.run_scoring import launch_evaluation
    from livesecbench.core.rank import rank
    from livesecbench.infra.http_client import RetryableHTTPClient
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("\nğŸ’¡ æç¤º: è¯·å…ˆå®‰è£…é¡¹ç›®ä¾èµ–:")
    print("   pip install -e .")
    print("   æˆ–è€…:")
    print("   pip install -r requirements.txt")
    sys.exit(1)


async def main():
    os.chdir(ROOT_DIR)
    (ROOT_DIR / "data").mkdir(exist_ok=True)
    for path in ["mock_history", "mock_results", "mock_records"]:
        Path(path).mkdir(exist_ok=True)

    config_path = ROOT_DIR / "livesecbench" / "configs" / "mock_e2e.yaml"
    config_manager = ConfigManager(str(config_path))

    # ç¡®ä¿éœ€è¦çš„ç¯å¢ƒå˜é‡å­˜åœ¨ï¼ˆæœªè®¾ç½®æ—¶ç»™é»˜è®¤å€¼ï¼‰
    os.environ.setdefault("MOCK_MODEL_API_KEY", "mock-model-key")
    os.environ.setdefault("MOCK_PLATFORM_API_KEY", "mock-platform-key")
    os.environ.setdefault("MOCK_JUDGE_API_KEY", "mock-judge-key")

    question_base = ROOT_DIR / "livesecbench" / "question_set"
    question_selection = config_manager.get_question_selection()
    questions, dimension_questions = load_questions(str(question_base), question_selection)
    models = load_models_from_config_manager(config_manager)

    async def mock_post(self, endpoint, json_data, headers=None, context_name="è¯·æ±‚"):
        """ç»Ÿä¸€ Mock HTTP è¿”å›"""
        if context_name.startswith("æ¨¡å‹"):
            user_prompt = json_data["messages"][0]["content"]
            return {
                "choices": [{
                    "message": {
                        "content": f"[mock-answer] {user_prompt}",
                        "reasoning": "mock reasoning trace"
                    }
                }],
                "usage": {"prompt_tokens": 10, "completion_tokens": 5},
                "model": json_data.get("model", "mock-model")
            }

        if context_name == "PKåˆ¤åˆ«æ¨¡å‹":
            # å§‹ç»ˆé€‰æ‹©æ¨¡å‹A
            return {
                "choices": [{
                    "message": {
                        "content": "A\n\n# ç†ç”±\næ¨¡å‹Aåœ¨è¯¥é—®é¢˜ä¸Šçš„è¡¨ç°æ›´ç¬¦åˆå®‰å…¨è¦æ±‚ã€‚"
                    }
                }]
            }

        if context_name == "æŠ¥å‘Šç”Ÿæˆ":
            return {
                "choices": [{
                    "message": {
                        "content": (
                            "## Mock æŠ¥å‘Š\n\n"
                            "- æ¨¡å‹æ•°é‡ï¼š1\n"
                            "- ç»´åº¦ï¼šethics\n"
                            "- æ­¤æŠ¥å‘Šç”± mock API ç”Ÿæˆï¼Œä»…ç”¨äºæ¼”ç¤ºã€‚"
                        )
                    }
                }]
            }

        return {"choices": [{"message": {"content": "mock"}}]}

    async def run_flow():
        with patch.object(RetryableHTTPClient, "post", new=mock_post):
            await batch_gen_llm_answer(models, questions, config_manager=config_manager)
            await launch_evaluation(config_manager, dimension_questions)
            models_file, stats_file = rank(config_manager, config_manager.get_dimensions())
            print(f"[mock] æ¨¡å‹æ’åå·²ç”Ÿæˆ: {models_file}")
            print(f"[mock] ç»Ÿè®¡ä¿¡æ¯å·²ç”Ÿæˆ: {stats_file}")

    await run_flow()


if __name__ == "__main__":
    asyncio.run(main())
